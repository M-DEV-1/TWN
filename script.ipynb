{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "\n",
    "# Initialize the MongoDB client\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "\n",
    "# Connect to the database and collection\n",
    "db = client['embedding_database']\n",
    "collection = db['bert_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File contents:\n",
      "#ID\tSynset_name\tPOS\tSynset_gloss  Prob_of_being_Past\tProb_of_being_Present\tProb_of_being_Future\tProb_of_being_Atemporal\n",
      "1740\table.a.01\ta\t(usually followed by `to') having the necessary means or skill or know-how or authority to do something\t0.0\t0.002\t0.998\t0\n",
      "2098\tunable.a.01\ta\t(usually followed by `to') not having the necessary means or skill or know-how\t0.0\t0.001798\t0.897202\t0.101\n",
      "2312\tabaxial.a.01\ta\tfacing away from the axis of an organ or organism\t0.004\t0.345\t0.651\t0\n",
      "2527\tadaxial.a.01\ta\tnearest to or facing toward the axis of an organ or organism\t0.002\t0.0\t0.998\t0\n"
     ]
    }
   ],
   "source": [
    "# Checks if the file is being read correctly. Returns the first 5 lines, including the titleheader\n",
    "\n",
    "def peek_file(file_path, num_lines=5):\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        for _ in range(num_lines):\n",
    "            \n",
    "            print(file.readline().strip())\n",
    "\n",
    "print(\"File contents:\")\n",
    "peek_file('./TempoWordNet/TempoWnL_1.0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117654 entries, 0 to 117653\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count   Dtype \n",
      "---  ------                   --------------   ----- \n",
      " 0   ID                       117654 non-null  object\n",
      " 1   Synset_name              117654 non-null  object\n",
      " 2   POS                      117654 non-null  object\n",
      " 3   Synset_gloss             117654 non-null  object\n",
      " 4   Prob_of_being_Past       117654 non-null  object\n",
      " 5   Prob_of_being_Present    117654 non-null  object\n",
      " 6   Prob_of_being_Future     117654 non-null  object\n",
      " 7   Prob_of_being_Atemporal  117654 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 7.2+ MB\n",
      "None\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "     ID      Synset_name POS  \\\n",
      "0  1740        able.a.01   a   \n",
      "1  2098      unable.a.01   a   \n",
      "2  2312     abaxial.a.01   a   \n",
      "3  2527     adaxial.a.01   a   \n",
      "4  2730  acroscopic.a.01   a   \n",
      "\n",
      "                                        Synset_gloss Prob_of_being_Past  \\\n",
      "0  (usually followed by `to') having the necessar...                0.0   \n",
      "1  (usually followed by `to') not having the nece...                0.0   \n",
      "2  facing away from the axis of an organ or organism              0.004   \n",
      "3  nearest to or facing toward the axis of an org...              0.002   \n",
      "4              facing or on the side toward the apex                0.0   \n",
      "\n",
      "  Prob_of_being_Present Prob_of_being_Future Prob_of_being_Atemporal  \n",
      "0                 0.002                0.998                       0  \n",
      "1              0.001798             0.897202                   0.101  \n",
      "2                 0.345                0.651                       0  \n",
      "3                   0.0                0.998                       0  \n",
      "4                 0.002                0.998                       0  \n"
     ]
    }
   ],
   "source": [
    "# Checks if the probabilities are being extracted properly\n",
    "\n",
    "def read_tempowordnet_manually(file_path):\n",
    "    # initiliase empty data list\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Skip comments\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # Manually split the line\n",
    "            # Strips leading and trailing whitespace from the line and then splits it into a list of words (or fields) using whitespace as the delimiter.\n",
    "            # split_line = line.strip().split()\n",
    "            split_line = line.strip().split('\\t')\n",
    "\n",
    "            # Extract the temporal probability fields from the end\n",
    "            # cannot find a better alternative to split_line >=8\n",
    "            if len(split_line) >= 8:\n",
    "                id_field = split_line[0]\n",
    "                synset_name_field = split_line[1]\n",
    "                pos_field = split_line[2]\n",
    "                \n",
    "                # Gloss is all fields between POS and the first probability field\n",
    "                gloss_field = ' '.join(split_line[3:-4])\n",
    "                \n",
    "                # Temporal probability fields\n",
    "                prob_past = split_line[-4]\n",
    "                prob_present = split_line[-3]\n",
    "                prob_future = split_line[-2]\n",
    "                prob_atemporal = split_line[-1]\n",
    "                \n",
    "                # Construct row data\n",
    "                row = {\n",
    "                    \"ID\": id_field,\n",
    "                    \"Synset_name\": synset_name_field,\n",
    "                    \"POS\": pos_field,\n",
    "                    \"Synset_gloss\": gloss_field,\n",
    "                    \"Prob_of_being_Past\": prob_past,\n",
    "                    \"Prob_of_being_Present\": prob_present,\n",
    "                    \"Prob_of_being_Future\": prob_future,\n",
    "                    \"Prob_of_being_Atemporal\": prob_atemporal\n",
    "                }\n",
    "                data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "tempowordnet_df = read_tempowordnet_manually('./TempoWordNet/TempoWnL_1.0.txt')\n",
    "\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(tempowordnet_df.info())\n",
    "print(\"\\nFirst few rows of the DataFrame:\")\n",
    "print(tempowordnet_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117654 entries, 0 to 117653\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   ID                       117654 non-null  object \n",
      " 1   Synset_name              117654 non-null  object \n",
      " 2   POS                      117654 non-null  object \n",
      " 3   Synset_gloss             117654 non-null  object \n",
      " 4   Prob_of_being_Past       117654 non-null  float64\n",
      " 5   Prob_of_being_Present    117654 non-null  float64\n",
      " 6   Prob_of_being_Future     117654 non-null  float64\n",
      " 7   Prob_of_being_Atemporal  117654 non-null  float64\n",
      " 8   Word                     117654 non-null  object \n",
      "dtypes: float64(4), object(5)\n",
      "memory usage: 8.1+ MB\n",
      "None\n",
      "\n",
      "First few rows of the updated DataFrame:\n",
      "     ID      Synset_name POS  \\\n",
      "0  1740        able.a.01   a   \n",
      "1  2098      unable.a.01   a   \n",
      "2  2312     abaxial.a.01   a   \n",
      "3  2527     adaxial.a.01   a   \n",
      "4  2730  acroscopic.a.01   a   \n",
      "\n",
      "                                        Synset_gloss  Prob_of_being_Past  \\\n",
      "0  (usually followed by `to') having the necessar...               0.000   \n",
      "1  (usually followed by `to') not having the nece...               0.000   \n",
      "2  facing away from the axis of an organ or organism               0.004   \n",
      "3  nearest to or facing toward the axis of an org...               0.002   \n",
      "4              facing or on the side toward the apex               0.000   \n",
      "\n",
      "   Prob_of_being_Present  Prob_of_being_Future  Prob_of_being_Atemporal  \\\n",
      "0               0.002000              0.998000                    0.000   \n",
      "1               0.001798              0.897202                    0.101   \n",
      "2               0.345000              0.651000                    0.000   \n",
      "3               0.000000              0.998000                    0.000   \n",
      "4               0.002000              0.998000                    0.000   \n",
      "\n",
      "         Word  \n",
      "0        able  \n",
      "1      unable  \n",
      "2     abaxial  \n",
      "3     adaxial  \n",
      "4  acroscopic  \n"
     ]
    }
   ],
   "source": [
    "# Create 'Word' column\n",
    "tempowordnet_df['Word'] = tempowordnet_df['Synset_name'].str.split('.').str[0]\n",
    "\n",
    "# Convert probability columns to numeric\n",
    "prob_columns = [\"Prob_of_being_Past\", \"Prob_of_being_Present\", \"Prob_of_being_Future\", \"Prob_of_being_Atemporal\"]\n",
    "for col in prob_columns:\n",
    "    tempowordnet_df[col] = pd.to_numeric(tempowordnet_df[col], errors='coerce')\n",
    "\n",
    "print(\"\\nUpdated DataFrame Info:\")\n",
    "print(tempowordnet_df.info())\n",
    "print(\"\\nFirst few rows of the updated DataFrame:\")\n",
    "print(tempowordnet_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pinecone\n",
    "import torch\n",
    "\n",
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize Pinecone service\n",
    "pinecone.init(api_key=API_KEY, environment='us-west1-gcp')\n",
    "\n",
    "index = pinecone.Index('tempowordnet') # what is this line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize BERT tokenizer and model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize lemmatizer and stemmer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Function for BERT embedding for a word\n",
    "def get_bert_embedding(word):\n",
    "    # Tokenize the word and convert it into input format suitable for BERT\n",
    "    encoded_input = tokenizer(word, return_tensors='pt')\n",
    "    # Pass the tokenized input through the BERT model to get the output embeddings\n",
    "    output = model(**encoded_input)\n",
    "    # Get the last hidden state and take the mean of the embeddings\n",
    "    return output.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "def insert_embedding_to_mongo(word):\n",
    "    # Get the embedding for the word\n",
    "    embedding = get_bert_embedding(word).tolist()[0]\n",
    "    # Create a document to insert\n",
    "    document = {\n",
    "        'word': word,\n",
    "        'embedding': embedding\n",
    "    }\n",
    "    # Insert the document into the MongoDB collection\n",
    "    collection.insert_one(document)\n",
    "\n",
    "# Function to read TempoWordNet manually\n",
    "def read_tempowordnet_manually(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            split_line = line.strip().split()  \n",
    "            if len(split_line) >= 8:\n",
    "                try:\n",
    "                    id_field = split_line[0]\n",
    "                    synset_name_field = split_line[1]\n",
    "                    pos_field = split_line[2]\n",
    "                    gloss_field = ' '.join(split_line[3:-4])\n",
    "                    prob_past = split_line[-4]\n",
    "                    prob_present = split_line[-3]\n",
    "                    prob_future = split_line[-2]\n",
    "                    prob_atemporal = split_line[-1]\n",
    "                    row = {\n",
    "                        \"ID\": id_field,\n",
    "                        \"Synset_name\": synset_name_field,\n",
    "                        \"POS\": pos_field,\n",
    "                        \"Synset_gloss\": gloss_field,\n",
    "                        \"Prob_of_being_Past\": prob_past,\n",
    "                        \"Prob_of_being_Present\": prob_present,\n",
    "                        \"Prob_of_being_Future\": prob_future,\n",
    "                        \"Prob_of_being_Atemporal\": prob_atemporal\n",
    "                    }\n",
    "                    data.append(row)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing line: {line}\")\n",
    "                    print(f\"Exception: {e}\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"Prob_of_being_Past\"] = pd.to_numeric(df[\"Prob_of_being_Past\"], errors='coerce')\n",
    "    df[\"Prob_of_being_Present\"] = pd.to_numeric(df[\"Prob_of_being_Present\"], errors='coerce')\n",
    "    df[\"Prob_of_being_Future\"] = pd.to_numeric(df[\"Prob_of_being_Future\"], errors='coerce')\n",
    "    df[\"Prob_of_being_Atemporal\"] = pd.to_numeric(df[\"Prob_of_being_Atemporal\"], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess the input text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # nltk.tokenize, split input text into tokens\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # filter out stopwords from the words list    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    # words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]\n",
    "    # older version where both lemmatization and stemming was happening\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "# Cosine Similarity Search Function\n",
    "def cosine_search(word_embedding, threshold=0.9):\n",
    "    \n",
    "    results = index.query(word_embedding, top_k=10)\n",
    "    \n",
    "    similar_words = []\n",
    "    \n",
    "    for match in results['matches']:\n",
    "        if match['score'] > threshold:\n",
    "            similar_words.append(match['id'])\n",
    "            \n",
    "    return similar_words\n",
    "\n",
    "\n",
    "# Calculate average temporal probabilities and extract IDs\n",
    "def calculate_temporal_probabilities(words, df):\n",
    "    temporal_probs = [] # to store tuples of temporal probabilities for each word\n",
    "    \n",
    "    ids = [] # id of each word used to calculate temp.prob\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            # case-sensitive substring search to find the word in the words list\n",
    "            matches = df[df['Synset_name'].str.contains(word)]\n",
    "            \n",
    "            if matches.empty:\n",
    "                # No exact match, use cosine similarity\n",
    "                \n",
    "                word_embedding = get_bert_embedding(word)\n",
    "                similar_words = cosine_search(word_embedding, threshold=0.9)\n",
    "                \n",
    "                if similar_words:\n",
    "                    for similar_word in similar_words:\n",
    "                        \n",
    "                        match = df[df['Word'] == similar_word]\n",
    "                        if not match.empty:\n",
    "                            probs = (\n",
    "                                match.iloc[0]['Prob_of_being_Past'],\n",
    "                                match.iloc[0]['Prob_of_being_Present'],\n",
    "                                match.iloc[0]['Prob_of_being_Future'],\n",
    "                                match.iloc[0]['Prob_of_being_Atemporal']\n",
    "                            )\n",
    "                            \n",
    "                            temporal_probs.append(probs)\n",
    "                            ids.append(match.iloc[0]['ID'])\n",
    "                            \n",
    "            else:\n",
    "                \n",
    "                for _, row in matches.iterrows():\n",
    "                    # tuple creation, and append to main lists\n",
    "                    probs = (row['Prob_of_being_Past'], row['Prob_of_being_Present'], row['Prob_of_being_Future'], row['Prob_of_being_Atemporal'])\n",
    "                    temporal_probs.append(probs)\n",
    "                    ids.append(row['ID'])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing word: {word}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "    \n",
    "    # if any probabilites are collected in temporal_probs list\n",
    "    if temporal_probs:\n",
    "        # return average probability and the id\n",
    "        # FUTURE!   consider changing average to weighted mean or any other method \n",
    "        avg_probs = np.mean(temporal_probs, axis=0)\n",
    "        return avg_probs, ids\n",
    "    else:\n",
    "        return None, []\n",
    "    \n",
    "    # Process the input sentence\n",
    "def process_sentence(sentence, df):\n",
    "    \n",
    "    words = preprocess_text(sentence)\n",
    "    \n",
    "    avg_probs, ids = calculate_temporal_probabilities(words, df)\n",
    "    \n",
    "    return words, avg_probs, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your TempoWordNet file\n",
    "file_path = './TempoWordNet/TempoWnL_1.0.txt'\n",
    "\n",
    "tempowordnet_df = read_tempowordnet_manually(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Insert BERT embeddings into Pinecone (once, you can run this once)\n",
    "#for word in tempowordnet_df['Word']:\n",
    "#     embedding = get_bert_embedding(word)\n",
    "#     index.upsert([(word, embedding)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sentence\n",
    "sentence = input('Input sentence for processing: ')\n",
    "\n",
    "# Process the sentence\n",
    "preprocessed_text, avg_temporal_probs, ids = process_sentence(sentence, tempowordnet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to processed_results.txt.\n"
     ]
    }
   ],
   "source": [
    "# Append results to a text file\n",
    "output_file_path = 'processed_results.txt'\n",
    "with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "    file.write(f\"\\n{'-'*50}\\n\")\n",
    "    file.write(f\"Original Sentence:\\n{sentence}\\n\\n\")\n",
    "    file.write(f\"Preprocessed Text:\\n{' '.join(preprocessed_text)}\\n\\n\")\n",
    "    if avg_temporal_probs is not None:\n",
    "        file.write(f\"{'ID':<15} {'Past':<10} {'Present':<10} {'Future':<10} {'Atemporal':<10}\\n\")\n",
    "        file.write(f\"{'-'*50}\\n\")\n",
    "        for id in ids:\n",
    "            match = tempowordnet_df[tempowordnet_df['ID'] == id].iloc[0]\n",
    "            file.write(f\"{id:<15} {match['Prob_of_being_Past']:<10.6f} {match['Prob_of_being_Present']:<10.6f} {match['Prob_of_being_Future']:<10.6f} {match['Prob_of_being_Atemporal']:<10.6f}\\n\")\n",
    "        file.write(f\"\\nAverage Temporal Probabilities:\\n\")\n",
    "        file.write(f\"Past: {avg_temporal_probs[0]:.6f}\\n\")\n",
    "        file.write(f\"Present: {avg_temporal_probs[1]:.6f}\\n\")\n",
    "        file.write(f\"Future: {avg_temporal_probs[2]:.6f}\\n\")\n",
    "        file.write(f\"Atemporal: {avg_temporal_probs[3]:.6f}\\n\")\n",
    "    else:\n",
    "        file.write(\"No valid words found in TempoWordNet.\\n\")\n",
    "\n",
    "print(f\"Results have been saved to {output_file_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
